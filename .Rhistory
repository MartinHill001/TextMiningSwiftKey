wdir
dir
getwd
getwd()
destfile="./SwiftKey/en_US/en_US.twitter.txt"
destfile.exists()
corpora="./SwiftKey/en_US/en_US.twitter.txt"
dataset:con <- file(corpora, "r")
corpora="../SwiftKey/en_US/en_US.twitter.txt"
dataset:con <- file(corpora, "r")
close(con)
corpora="./SwiftKey/en_US/en_US.twitter.txt"
dataset:con <- file(corpora, "r")
getwd()
samples <-  c('twitter', 'blogs', 'news')
#WEEK 1 - Task 2 - Profanity filter and create list containing each sample
sample_list <- list()
for(i in 1:length(samples))
{
sample_name<-samples[i]
tokenfile= cat("./TextMiningSwiftKey/token.",sample_name,".txt" , rm.space = TRUE)
}
tokenfile
samples <-  c('twitter', 'blogs', 'news')
#WEEK 1 - Task 2 - Profanity filter and create list containing each sample
sample_list <- list()
for(i in 1:length(samples))
{
sample_name<-samples[i]
tokenfile= paste("./TextMiningSwiftKey/token.",sample_name,".txt" , rm.space = TRUE)
}
tokenfile
samples <-  c('twitter', 'blogs', 'news')
#WEEK 1 - Task 2 - Profanity filter and create list containing each sample
sample_list <- list()
for(i in 1:length(samples))
{
sample_name<-samples[i]
tokenfile= paste("./TextMiningSwiftKey/token.",sample_name,".txt" , sep = "")
}
tokenfile
tokenfile
knitr::opts_chunk$set(echo = TRUE)
samples <-  c('twitter', 'blogs', 'news')
#WEEK 1 - Task 2 - Profanity filter and create list containing each sample
sample_list <- list()
for(i in 1:length(samples))
{
sample_name<-samples[i]
tokenfile= paste("./TextMiningSwiftKey/token.",sample_name,".txt" , sep = "")
#read token data
tokenConn <- file(tokenfile, "r")
tokenlist <- readLines(con, warn = FALSE, encoding = "UTF-8")
close(tokenConn)
#remove row where there are rude words - removing singular words may cause false associations
#Note : I'm sure there are lots of rude words to add - but let's not list them all yet - just as they are discovered
tokenlist <- tokenlist[!grepl("(bullshit|fuck)", tokenlist, ignore.case = TRUE)]
#single string for tokenization
token <- paste(tokenlist, collapse = ' ')
#add to list
sample_list <- append(sample_list, token)
}
samples <-  c('twitter', 'blogs', 'news')
#WEEK 1 - Task 2 - Profanity filter and create list containing each sample
sample_list <- list()
for(i in 1:length(samples))
{
sample_name<-samples[i]
tokenfile= paste("./TextMiningSwiftKey/token.",sample_name,".txt" , sep = "")
#read token data
tokenConn <- file(tokenfile, "r")
tokenlist <- readLines(con, warn = FALSE, encoding = "UTF-8")
close(tokenConn)
#remove row where there are rude words - removing singular words may cause false associations
#Note : I'm sure there are lots of rude words to add - but let's not list them all yet - just as they are discovered
tokenlist <- tokenlist[!grepl("(bullshit|fuck)", tokenlist, ignore.case = TRUE)]
#single string for tokenization
token <- paste(tokenlist, collapse = ' ')
#add to list
sample_list <- append(sample_list, token)
}
getwd()
